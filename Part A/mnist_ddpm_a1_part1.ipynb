{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "OimlcBLxYkqc",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# This UNET-style prediction model was originally included as part of the Score-based generative modelling tutorial \n",
    "# by Yang Song et al: https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "class GaussianFourierProjection(nn.Module):\n",
    "  \"\"\"Gaussian random features for encoding time steps.\"\"\"  \n",
    "  def __init__(self, embed_dim, scale=30.):\n",
    "    super().__init__()\n",
    "    # Randomly sample weights during initialization. These weights are fixed \n",
    "    # during optimization and are not trainable.\n",
    "    self.W = nn.Parameter(torch.randn(embed_dim // 2) * scale, requires_grad=False)\n",
    "  def forward(self, x):\n",
    "    x_proj = x[:, None] * self.W[None, :] * 2 * np.pi\n",
    "    return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
    "\n",
    "\n",
    "class Dense(nn.Module):\n",
    "  \"\"\"A fully connected layer that reshapes outputs to feature maps.\"\"\"\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.dense = nn.Linear(input_dim, output_dim)\n",
    "  def forward(self, x):\n",
    "    return self.dense(x)[..., None, None]\n",
    "\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "  \"\"\"A time-dependent score-based model built upon U-Net architecture.\"\"\"\n",
    "\n",
    "  def __init__(self, marginal_prob_std, channels=[32, 64, 128, 256], embed_dim=256):\n",
    "    \"\"\"Initialize a time-dependent score-based network.\n",
    "\n",
    "    Args:\n",
    "      marginal_prob_std: A function that takes time t and gives the standard\n",
    "        deviation of the perturbation kernel p_{0t}(x(t) | x(0)).\n",
    "      channels: The number of channels for feature maps of each resolution.\n",
    "      embed_dim: The dimensionality of Gaussian random feature embeddings.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    # Gaussian random feature embedding layer for time\n",
    "    self.embed = nn.Sequential(GaussianFourierProjection(embed_dim=embed_dim),\n",
    "         nn.Linear(embed_dim, embed_dim))\n",
    "    # Encoding layers where the resolution decreases\n",
    "    self.conv1 = nn.Conv2d(1, channels[0], 3, stride=1, bias=False)\n",
    "    self.dense1 = Dense(embed_dim, channels[0])\n",
    "    self.gnorm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
    "    self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride=2, bias=False)\n",
    "    self.dense2 = Dense(embed_dim, channels[1])\n",
    "    self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "    self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
    "    self.dense3 = Dense(embed_dim, channels[2])\n",
    "    self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "    self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
    "    self.dense4 = Dense(embed_dim, channels[3])\n",
    "    self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])    \n",
    "\n",
    "    # Decoding layers where the resolution increases\n",
    "    self.tconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, bias=False)\n",
    "    self.dense5 = Dense(embed_dim, channels[2])\n",
    "    self.tgnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
    "    self.tconv3 = nn.ConvTranspose2d(channels[2] + channels[2], channels[1], 3, stride=2, bias=False, output_padding=1)    \n",
    "    self.dense6 = Dense(embed_dim, channels[1])\n",
    "    self.tgnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
    "    self.tconv2 = nn.ConvTranspose2d(channels[1] + channels[1], channels[0], 3, stride=2, bias=False, output_padding=1)    \n",
    "    self.dense7 = Dense(embed_dim, channels[0])\n",
    "    self.tgnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
    "    self.tconv1 = nn.ConvTranspose2d(channels[0] + channels[0], 1, 3, stride=1)\n",
    "    \n",
    "    # The swish activation function\n",
    "    self.act = lambda x: x * torch.sigmoid(x)\n",
    "    self.marginal_prob_std = marginal_prob_std\n",
    "  \n",
    "  def forward(self, x, t): \n",
    "    # Obtain the Gaussian random feature embedding for t   \n",
    "    embed = self.act(self.embed(t))    \n",
    "    # Encoding path\n",
    "    h1 = self.conv1(x)    \n",
    "    ## Incorporate information from t\n",
    "    h1 += self.dense1(embed)\n",
    "    ## Group normalization\n",
    "    h1 = self.gnorm1(h1)\n",
    "    h1 = self.act(h1)\n",
    "    h2 = self.conv2(h1)\n",
    "    h2 += self.dense2(embed)\n",
    "    h2 = self.gnorm2(h2)\n",
    "    h2 = self.act(h2)\n",
    "    h3 = self.conv3(h2)\n",
    "    h3 += self.dense3(embed)\n",
    "    h3 = self.gnorm3(h3)\n",
    "    h3 = self.act(h3)\n",
    "    h4 = self.conv4(h3)\n",
    "    h4 += self.dense4(embed)\n",
    "    h4 = self.gnorm4(h4)\n",
    "    h4 = self.act(h4)\n",
    "\n",
    "    # Decoding path\n",
    "    h = self.tconv4(h4)\n",
    "    ## Skip connection from the encoding path\n",
    "    h += self.dense5(embed)\n",
    "    h = self.tgnorm4(h)\n",
    "    h = self.act(h)\n",
    "    h = self.tconv3(torch.cat([h, h3], dim=1))\n",
    "    h += self.dense6(embed)\n",
    "    h = self.tgnorm3(h)\n",
    "    h = self.act(h)\n",
    "    h = self.tconv2(torch.cat([h, h2], dim=1))\n",
    "    h += self.dense7(embed)\n",
    "    h = self.tgnorm2(h)\n",
    "    h = self.act(h)\n",
    "    h = self.tconv1(torch.cat([h, h1], dim=1))\n",
    "\n",
    "    # Normalize output\n",
    "    h = h / self.marginal_prob_std(t)[:, None, None, None]\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ExponentialMovingAverage implementation as used in pytorch vision\n",
    "# https://github.com/pytorch/vision/blob/main/references/classification/utils.py#L159\n",
    "\n",
    "# BSD 3-Clause License\n",
    "\n",
    "# Copyright (c) Soumith Chintala 2016, \n",
    "# All rights reserved.\n",
    "\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "\n",
    "# * Redistributions of source code must retain the above copyright notice, this\n",
    "#   list of conditions and the following disclaimer.\n",
    "\n",
    "# * Redistributions in binary form must reproduce the above copyright notice,\n",
    "#   this list of conditions and the following disclaimer in the documentation\n",
    "#   and/or other materials provided with the distribution.\n",
    "\n",
    "# * Neither the name of the copyright holder nor the names of its\n",
    "#   contributors may be used to endorse or promote products derived from\n",
    "#   this software without specific prior written permission.\n",
    "\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
    "# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
    "# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
    "# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
    "# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
    "# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
    "# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
    "# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
    "    \n",
    "class ExponentialMovingAverage(torch.optim.swa_utils.AveragedModel):\n",
    "    \"\"\"Maintains moving averages of model parameters using an exponential decay.\n",
    "    ``ema_avg = decay * avg_model_param + (1 - decay) * model_param``\n",
    "    `torch.optim.swa_utils.AveragedModel <https://pytorch.org/docs/stable/optim.html#custom-averaging-strategies>`_\n",
    "    is used to compute the EMA.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay, device=\"cpu\"):\n",
    "        def ema_avg(avg_model_param, model_param, num_averaged):\n",
    "            return decay * avg_model_param + (1 - decay) * model_param\n",
    "\n",
    "        super().__init__(model, device, ema_avg, use_buffers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "645d91e4bb974b1196be61b5077c9dc5",
      "78dc714c7aa347fb9fc41abf420222d9",
      "c1260f271df547fbb2a158ff6b3a3ff4",
      "e7313fdbb70442f4867644dfc85c3bcc",
      "a501588b5eb0494996dfb136565365ca",
      "89c68eded05d441daf94d145addb5ece",
      "2bffd3855f5744f588d5be1e5c4aed3e",
      "3b61ee9c62994863b718c086d4182f44",
      "8b905c5b2ad846ca837bd20cce2bf094",
      "b1416c32c4af4fe9a3c3fdcc5f33aca0",
      "aca161ff9f4b4a20b1457a8ee864f150"
     ]
    },
    "id": "mcoxR2ajYkqe",
    "outputId": "1f39bd8e-e78c-42e6-89cc-f1df34bdbdea"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcusdreisler/miniconda3/envs/PML/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms, utils\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from diffusion_utils import gaussian_nll, pred_xstart_from_eps\n",
    "from functions import normal_kl, discretized_gaussian_loglik, flat_mean\n",
    "\n",
    "class DDPM(nn.Module):\n",
    "\n",
    "    def __init__(self, network, T=100, beta_1=1e-4, beta_T=2e-2, x0_parameterization=False, use_low_discrepancy_sampler=False):\n",
    "        \"\"\"\n",
    "        Initialize Denoising Diffusion Probabilistic Model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        network: nn.Module\n",
    "            The inner neural network used by the diffusion process. Typically a Unet.\n",
    "        beta_1: float\n",
    "            beta_t value at t=1 \n",
    "        beta_T: [float]\n",
    "            beta_t value at t=T (last step)\n",
    "        T: int\n",
    "            The number of diffusion steps.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(DDPM, self).__init__()\n",
    "\n",
    "        self.x0_parameterization = x0_parameterization\n",
    "        self.use_low_discrepancy_sampler = use_low_discrepancy_sampler\n",
    "\n",
    "        # Normalize time input before evaluating neural network\n",
    "        # Reshape input into image format and normalize time value before sending it to network model\n",
    "        self._network = network\n",
    "        self.network = lambda x, t: (self._network(x.reshape(-1, 1, 28, 28), \n",
    "                                                   (t.squeeze()/T))\n",
    "                                    ).reshape(-1, 28*28)\n",
    "\n",
    "        # Total number of time steps\n",
    "        self.T = T\n",
    "\n",
    "        # Registering as buffers to ensure they get transferred to the GPU automatically\n",
    "        self.register_buffer(\"beta\", torch.linspace(beta_1, beta_T, T+1))\n",
    "        self.register_buffer(\"alpha\", 1-self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", self.alpha.cumprod(dim=0))\n",
    "        self.register_buffer(\"beta_bar\", self.beta.cumprod(dim=0))\n",
    "        self.register_buffer(\"sqrt_alphas_bar\", torch.sqrt(self.alpha_bar))\n",
    "        self.register_buffer(\"sqrt_one_minus_alphas_bar\", torch.sqrt(1-self.alpha_bar))\n",
    "\n",
    "        # q(x_{t-1} | x_t, x_0)\n",
    "        #clever having two alphas with and without s but the more the merrier\n",
    "        self.alphas_bar = self.alpha_bar\n",
    "        self.betas = self.beta\n",
    "        self.alphas = self.alpha\n",
    "        self.betas_bar = self.beta_bar\n",
    "        alphas_bar_prev = torch.cat([torch.as_tensor([1., ], dtype=torch.float64), self.alpha_bar[:-1]])\n",
    "        sqrt_alphas_bar_prev = torch.sqrt(alphas_bar_prev)\n",
    "        self.sqrt_recip_alphas_bar = torch.sqrt(1. / self.alphas_bar)\n",
    "        self.sqrt_recip_m1_alphas_bar = torch.sqrt(1. / self.alphas_bar - 1.)  # m1: minus 1\n",
    "        self.posterior_var = self.betas * (1. - alphas_bar_prev) / (1. - self.alphas_bar)\n",
    "        self.posterior_logvar_clipped = torch.log(torch.cat([self.posterior_var[[1]], self.posterior_var[1:]]))\n",
    "        self.posterior_mean_coef1 = self.betas * sqrt_alphas_bar_prev / (1. - self.alphas_bar)\n",
    "        self.posterior_mean_coef2 = torch.sqrt(self.alphas) * (1. - alphas_bar_prev) / (1. - self.alphas_bar)\n",
    "        \n",
    "\n",
    "    def forward_diffusion(self, x0, t, epsilon):\n",
    "        '''\n",
    "        q(x_t | x_0)\n",
    "        Forward diffusion from an input datapoint x0 to an xt at timestep t, provided a N(0,1) noise sample epsilon. \n",
    "        Note that we can do this operation in a single step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            x value at t=0 (an input image)\n",
    "        t: int\n",
    "            step index \n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t\n",
    "        ''' \n",
    "\n",
    "        mean = torch.sqrt(self.alpha_bar[t])*x0\n",
    "        std = torch.sqrt(1 - self.alpha_bar[t])\n",
    "        \n",
    "        return mean + std*epsilon\n",
    "\n",
    "    def reverse_diffusion(self, xt, t, epsilon):\n",
    "        \"\"\"\n",
    "        p(x_{t-1} | x_t)\n",
    "        Single step in the reverse direction, from x_t (at timestep t) to x_{t-1}, provided a N(0,1) noise sample epsilon.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xt: torch.tensor\n",
    "            x value at step t\n",
    "        t: int\n",
    "            step index\n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t-1\n",
    "        \"\"\"\n",
    "        \n",
    "        #equation 11 in Ho et al, 2020\n",
    "        mean =  1./torch.sqrt(self.alpha[t]) * (xt - (self.beta[t])/torch.sqrt(1-self.alpha_bar[t])*self.network(xt, t)) \n",
    "\n",
    "        #std sounds more like an art: \"Experimentally, both σt2 = βt and σ2 = β ̃ = 1−α ̄t−1 β had similar results.\"\"\n",
    "        std = torch.where(t>0, torch.sqrt(((1-self.alpha_bar[t-1]) / (1-self.alpha_bar[t]))*self.beta[t]), 0)\n",
    "        \n",
    "        return mean + std*epsilon\n",
    "\n",
    "    def reverse_diffusion_x0_parameterization(self, xt, t, epsilon):\n",
    "        \"\"\"\n",
    "        p(x_{t-1} | x_t)\n",
    "        Single step in the reverse direction, from x_t (at timestep t) to x_{t-1}, provided a N(0,1) noise sample epsilon.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xt: torch.tensor\n",
    "            x value at step t\n",
    "        t: int or torch.tensor\n",
    "            step index\n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t-1\n",
    "        \"\"\"\n",
    "\n",
    "        # Network now predicts the initial image x0\n",
    "        estimated_x0 = self.network(xt, t)\n",
    "\n",
    "        alpha = self.alpha[t]\n",
    "        alpha_bar = self.alpha_bar[t]\n",
    "        alpha_bar_prev = self.alpha_bar[t-1]\n",
    "        beta = self.beta[t]\n",
    "\n",
    "        # Equation 6+7 in Ho et al, 2020\n",
    "\n",
    "        beta_tilde = ((1-alpha_bar_prev)/(1-alpha_bar))*beta\n",
    "        std = torch.sqrt(beta_tilde)\n",
    "\n",
    "        coeff1 = torch.sqrt(alpha_bar_prev)/(1-alpha_bar)*beta\n",
    "        coeff2 = torch.sqrt(alpha)*(1-alpha_bar_prev)/(1-alpha_bar)\n",
    "\n",
    "        mean = coeff1*estimated_x0 + coeff2*xt\n",
    "        \n",
    "        return mean + std * epsilon\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, shape):\n",
    "        \"\"\"\n",
    "        Sample from diffusion model (Algorithm 2 in Ho et al, 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape: tuple\n",
    "            Specify shape of sampled output. For MNIST: (nsamples, 28*28)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            sampled image            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Sample xT: Gaussian noise\n",
    "        xT = torch.randn(shape).to(self.beta.device)\n",
    "\n",
    "        xt = xT\n",
    "        for t in range(self.T, 0, -1):\n",
    "            noise = torch.randn_like(xT) if t > 1 else 0\n",
    "            t = torch.tensor(t).expand(xt.shape[0], 1).to(self.beta.device)   \n",
    "            if self.x0_parameterization:\n",
    "                xt = self.reverse_diffusion_x0_parameterization(xt, t, noise)\n",
    "            else:         \n",
    "                xt = self.reverse_diffusion(xt, t, noise)\n",
    "\n",
    "        return xt\n",
    "\n",
    "    def low_discrepancy_t_sampler(self, batch_size, device):\n",
    "        k = batch_size\n",
    "        u0 = np.random.uniform(0, 1)\n",
    "\n",
    "        # ti = mod(u0 + i/k, 1) for i=0,1,...,k-1\n",
    "        ti = torch.fmod(torch.arange(0, k, device=device)/k + u0, 1)\n",
    "        return (ti*self.T).long().unsqueeze(1)\n",
    "\n",
    "    \n",
    "    def elbo_simple(self, x0):\n",
    "        \"\"\"\n",
    "        ELBO training objective (Algorithm 1 in Ho et al, 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            Input image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            ELBO value            \n",
    "        \"\"\"\n",
    "\n",
    "        if self.use_low_discrepancy_sampler:\n",
    "            t = self.low_discrepancy_t_sampler(x0.shape[0], x0.device)\n",
    "        else:\n",
    "            # Sample time step t\n",
    "            t = torch.randint(1, self.T, (x0.shape[0],1)).to(x0.device)\n",
    "\n",
    "        \n",
    "        # Sample noise\n",
    "        epsilon = torch.randn_like(x0)\n",
    "\n",
    "        # TODO: Forward diffusion to produce image at step t\n",
    "        xt = self.forward_diffusion(x0, t, epsilon)\n",
    "        \n",
    "        return -nn.MSELoss(reduction='mean')(epsilon, self.network(xt, t))\n",
    "\n",
    "\n",
    "    def elbo_simple_x0_reparameterization(self, x0):\n",
    "        \"\"\"\n",
    "        ELBO training objective (Algorithm 1 in Ho et al, 2020), modified. Network tries to predict the noise, but the loss is taken in \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            Input image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            ELBO value            \n",
    "        \"\"\"\n",
    "\n",
    "        # Sample time step t\n",
    "        t = torch.randint(1, self.T, (x0.shape[0],1)).to(x0.device)\n",
    "        \n",
    "        # Sample noise\n",
    "        epsilon = torch.randn_like(x0)\n",
    "\n",
    "        # TODO: Forward diffusion to produce image at step t\n",
    "        xt = self.forward_diffusion(x0, t, epsilon)\n",
    "\n",
    "        estimated_x0 = self.network(xt, t)\n",
    "        \n",
    "        return -nn.MSELoss(reduction='mean')(x0, estimated_x0)\n",
    "\n",
    "    \n",
    "    def loss(self, x0):\n",
    "        \"\"\"\n",
    "        Loss function. Just the negative of the ELBO.\n",
    "        \"\"\"\n",
    "        if self.x0_parameterization:\n",
    "            return -self.elbo_simple_x0_reparameterization(x0).mean()\n",
    "        else:\n",
    "            return -self.elbo_simple(x0).mean()\n",
    "\n",
    "    \n",
    "    # === log likelihood ===\n",
    "    # bpd: bits per dimension\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract(\n",
    "            arr, t, x,\n",
    "            dtype=torch.float32, device=torch.device(\"cpu\"), ndim=4):\n",
    "        if x is not None:\n",
    "            dtype = x.dtype\n",
    "            device = x.device\n",
    "            ndim = x.ndim\n",
    "        out = torch.as_tensor(arr, dtype=dtype, device=device).gather(0, t)\n",
    "        return out.reshape((-1, ) + (1, ) * (ndim - 1))\n",
    "\n",
    "    def q_mean_var(self, x_0, t):\n",
    "        mean = self._extract(self.sqrt_alphas_bar, t, x_0) * x_0\n",
    "        var = self._extract(1. - self.alphas_bar, t, x_0)\n",
    "        logvar = self._extract(torch.log(1 - self.alphas_bar), t, x_0)\n",
    "        return mean, var, logvar\n",
    "\n",
    "    def q_sample(self, x_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        coef1 = self._extract(self.sqrt_alphas_bar, t, x_0)\n",
    "        coef2 = self._extract(self.sqrt_one_minus_alphas_bar, t, x_0)\n",
    "        return coef1 * x_0 + coef2 * noise\n",
    "\n",
    "    def q_posterior_mean_var(self, x_0, x_t, t):\n",
    "        posterior_mean_coef1 = self._extract(self.posterior_mean_coef1, t, x_0)\n",
    "        posterior_mean_coef2 = self._extract(self.posterior_mean_coef2, t, x_0)\n",
    "        posterior_mean = posterior_mean_coef1 * x_0 + posterior_mean_coef2 * x_t\n",
    "        posterior_var = self._extract(self.posterior_var, t, x_0)\n",
    "        posterior_logvar = self._extract(self.posterior_logvar_clipped, t, x_0)\n",
    "        return posterior_mean, posterior_var, posterior_logvar\n",
    "\n",
    "    def _loss_term_bpd(self, denoise_fn, x_0, x_t, t, clip_denoised, return_pred):\n",
    "        # calculate L_t\n",
    "        # t = 0: negative log likelihood of decoder, -\\log p(x_0 | x_1)\n",
    "        # t > 0: variational lower bound loss term, KL term\n",
    "        true_mean, _, true_logvar = self.q_posterior_mean_var(x_0=x_0, x_t=x_t, t=t)\n",
    "        model_mean, _, model_logvar, pred_x_0 = self.p_mean_var(x_t=x_t, t=t, clip_denoised=clip_denoised, return_pred=True)\n",
    "        kl = normal_kl(true_mean, true_logvar, model_mean, model_logvar)\n",
    "        kl = flat_mean(kl) / math.log(2.)  # natural base to base 2\n",
    "        decoder_nll = discretized_gaussian_loglik(x_0, model_mean, log_scale=0.5 * model_logvar).neg()\n",
    "        decoder_nll = flat_mean(decoder_nll) / math.log(2.)\n",
    "        output = torch.where(t.to(kl.device) > 0, kl, decoder_nll)\n",
    "        return (output, pred_x_0) if return_pred else output\n",
    "\n",
    "    \n",
    "    def p_mean_var(self, x_t, t, clip_denoised=True, return_pred=False):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of p(x_{t-1} | x_t)\n",
    "        \"\"\"\n",
    "        if self.x0_parameterization:\n",
    "            # Network predicts x0 directly\n",
    "            pred_x0 = self.network(x_t, t)\n",
    "            if clip_denoised:\n",
    "                pred_x0 = pred_x0.clamp(-1., 1.)\n",
    "            \n",
    "            # Compute the posterior mean and variance using the predicted x0\n",
    "            mean, var, logvar = self.q_posterior_mean_var(x_0=pred_x0, x_t=x_t, t=t)\n",
    "        else:\n",
    "            # Network predicts the noise epsilon\n",
    "            epsilon_theta = self.network(x_t, t)\n",
    "            \n",
    "            # Compute the mean of p(x_{t-1} | x_t) using the predicted epsilon\n",
    "            mean = (1. / torch.sqrt(self.alpha[t])) * (\n",
    "                x_t - (self.beta[t] / torch.sqrt(1 - self.alpha_bar[t])) * epsilon_theta\n",
    "            )\n",
    "            \n",
    "            # Extract the variance and log variance for p(x_{t-1} | x_t)\n",
    "            var = self.posterior_var[t]\n",
    "            logvar = self.posterior_logvar_clipped[t]\n",
    "            \n",
    "            if clip_denoised:\n",
    "                # Estimate x0 from the predicted epsilon\n",
    "                pred_x0 = (x_t - torch.sqrt(1 - self.alpha_bar[t]) * epsilon_theta) / torch.sqrt(self.alpha_bar[t])\n",
    "                pred_x0 = pred_x0.clamp(-1., 1.)\n",
    "            else:\n",
    "                pred_x0 = None\n",
    "\n",
    "        if return_pred:\n",
    "            return mean, var, logvar, pred_x0\n",
    "        else:\n",
    "            return mean, var, logvar\n",
    "\n",
    "\n",
    "\n",
    "    def train_losses(self, denoise_fn, x_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        x_t = self.q_sample(x_0, t, noise=noise)\n",
    "\n",
    "        # calculate the loss\n",
    "        # kl: weighted\n",
    "        # mse: unweighted\n",
    "        if self.loss_type == \"kl\":\n",
    "            losses = self._loss_term_bpd(\n",
    "                denoise_fn, x_0=x_0, x_t=x_t, t=t, clip_denoised=False, return_pred=False)\n",
    "        elif self.loss_type == \"mse\":\n",
    "            assert self.model_var_type != \"learned\"\n",
    "            if self.model_mean_type == \"mean\":\n",
    "                target = self.q_posterior_mean_var(x_0=x_0, x_t=x_t, t=t)[0]\n",
    "            elif self.model_mean_type == \"x_0\":\n",
    "                target = x_0\n",
    "            elif self.model_mean_type == \"eps\":\n",
    "                target = noise\n",
    "            else:\n",
    "                raise NotImplementedError(self.model_mean_type)\n",
    "            model_out = denoise_fn(x_t, t)\n",
    "            losses = flat_mean((target - model_out).pow(2))\n",
    "        else:\n",
    "            raise NotImplementedError(self.loss_type)\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def _prior_bpd(self, x_0):\n",
    "        B, T = len(x_0), self.timesteps\n",
    "        T_mean, _, T_logvar = self.q_mean_var(\n",
    "            x_0=x_0, t=(T - 1) * torch.ones((B, ), dtype=torch.int64))\n",
    "        kl_prior = normal_kl(T_mean, T_logvar, mean2=0., logvar2=0.)\n",
    "        return flat_mean(kl_prior) / math.log(2.)\n",
    "\n",
    "    def calc_all_bpd(self, denoise_fn, x_0, clip_denoised=True):\n",
    "        B = x_0.shape[0]  # Ensure B is the batch size\n",
    "        T = self.T\n",
    "        t = torch.empty((B,), dtype=torch.int64)  # Use tuple for size\n",
    "        losses = torch.zeros((B, T), dtype=torch.float32)  # Use tuple for size\n",
    "        mses = torch.zeros((B, T), dtype=torch.float32)  \n",
    "\n",
    "        for ti in range(T - 1, -1, -1):\n",
    "            t.fill_(ti)\n",
    "            x_t = self.q_sample(x_0, t=t)\n",
    "            loss, pred_x_0 = self._loss_term_bpd(\n",
    "                denoise_fn, x_0, x_t=x_t, t=t, clip_denoised=clip_denoised, return_pred=True)\n",
    "            losses[:, ti] = loss\n",
    "            mses[:, ti] = flat_mean((pred_x_0 - x_0).pow(2))\n",
    "\n",
    "        prior_bpd = self._prior_bpd(x_0)\n",
    "        total_bpd = torch.sum(losses, dim=1) + prior_bpd\n",
    "        return total_bpd, losses, prior_bpd, mses\n",
    "\n",
    "\n",
    "\n",
    "def train(model, optimizer, scheduler, dataloader, epochs, device, ema=True, per_epoch_callback=None):\n",
    "    \"\"\"\n",
    "    Training loop\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        Pytorch model\n",
    "    optimizer: optim.Optimizer\n",
    "        Pytorch optimizer to be used for training\n",
    "    scheduler: optim.LRScheduler\n",
    "        Pytorch learning rate scheduler\n",
    "    dataloader: utils.DataLoader\n",
    "        Pytorch dataloader\n",
    "    epochs: int\n",
    "        Number of epochs to train\n",
    "    device: torch.device\n",
    "        Pytorch device specification\n",
    "    ema: Boolean\n",
    "        Whether to activate Exponential Model Averaging\n",
    "    per_epoch_callback: function\n",
    "        Called at the end of every epoch\n",
    "    \"\"\"\n",
    "\n",
    "    # Setup progress bar\n",
    "    total_steps = len(dataloader)*epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training\")\n",
    "\n",
    "    if ema:\n",
    "        ema_global_step_counter = 0\n",
    "        ema_steps = 10\n",
    "        ema_adjust = dataloader.batch_size * ema_steps / epochs\n",
    "        ema_decay = 1.0 - 0.995\n",
    "        ema_alpha = min(1.0, (1.0 - ema_decay) * ema_adjust)\n",
    "        ema_model = ExponentialMovingAverage(model, device=device, decay=1.0 - ema_alpha)                \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Switch to train mode\n",
    "        model.train()\n",
    "\n",
    "        global_step_counter = 0\n",
    "        for i, (x, _) in enumerate(dataloader):\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():12.4f}\", epoch=f\"{epoch+1}/{epochs}\", lr=f\"{scheduler.get_last_lr()[0]:.2E}\")\n",
    "            progress_bar.update()\n",
    "\n",
    "            if ema:\n",
    "                ema_global_step_counter += 1\n",
    "                if ema_global_step_counter%ema_steps==0:\n",
    "                    ema_model.update_parameters(model)                \n",
    "        \n",
    "        if per_epoch_callback:\n",
    "            per_epoch_callback(ema_model.module if ema else model)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0: \n",
    "            torch.save(model.state_dict(), f\"model_discrete_ddpm_baseline_2_{epoch}_.pt\")\n",
    "\n",
    "\n",
    "# Parameters\n",
    "T = 1000\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "# Rather than treating MNIST images as discrete objects, as done in Ho et al 2020, \n",
    "# we here treat them as continuous input data, by dequantizing the pixel values (adding noise to the input data)\n",
    "# Also note that we map the 0..255 pixel values to [-1, 1], and that we process the 28x28 pixel values as a flattened 784 tensor.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Lambda(lambda x: x + torch.rand(x.shape)/255),    # Dequantize pixel values\n",
    "    transforms.Lambda(lambda x: (x-0.5)*2.0),                    # Map from [0,1] -> [-1, -1]\n",
    "    transforms.Lambda(lambda x: x.flatten())\n",
    "])\n",
    "\n",
    "# Download and transform train dataset\n",
    "dataloader_train = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data', download=True, train=True, transform=transform),\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "\n",
    "# Select device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Construct Unet\n",
    "# The original ScoreNet expects a function with std for all the\n",
    "# different noise levels, such that the output can be rescaled.\n",
    "# Since we are predicting the noise (rather than the score), we\n",
    "# ignore this rescaling and just set std=1 for all t.\n",
    "mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "# Construct model\n",
    "model = DDPM(mnist_unet, T=T, x0_parameterization=False, use_low_discrepancy_sampler=True).to(device)\n",
    "\n",
    "# Construct optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Setup simple scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.9999)\n",
    "\n",
    "\n",
    "def reporter(model):\n",
    "    \"\"\"Callback function used for plotting images during training\"\"\"\n",
    "    \n",
    "    # Switch to eval mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        nsamples = 10\n",
    "        samples = model.sample((nsamples,28*28)).cpu()\n",
    "        \n",
    "        # Map pixel values back from [-1,1] to [0,1]\n",
    "        samples = (samples+1)/2 \n",
    "        samples = samples.clamp(0.0, 1.0)\n",
    "\n",
    "        # Plot in grid\n",
    "        grid = utils.make_grid(samples.reshape(-1, 1, 28, 28), nrow=nsamples)\n",
    "        plt.gca().set_axis_off()\n",
    "        plt.imshow(transforms.functional.to_pil_image(grid), cmap=\"gray\")\n",
    "        plt.show()   \n",
    "\n",
    "\n",
    "#train(model, optimizer, scheduler, dataloader_train, \n",
    "#      epochs=epochs, device=device, ema=True, per_epoch_callback=reporter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call training loop\n",
    "#train(model, optimizer, scheduler, dataloader_train, \n",
    "#      epochs=epochs, device=device, ema=True, per_epoch_callback=reporter)\n",
    "\n",
    "from metrics import calculate_inception_score_and_fid\n",
    "\n",
    "def evaluate(dataloader, DDPM_class, model_ckpt, device, nsamples=10000):\n",
    "    \"\"\"\n",
    "    Evaluate model using Inception Score and FID\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataloader: utils.DataLoader\n",
    "        Pytorch dataloader\n",
    "    model: nn.Module\n",
    "        Pytorch model\n",
    "    model_ckpt: str\n",
    "        Path to model checkpoint\n",
    "    device: torch.device\n",
    "        Pytorch device specification\n",
    "    nsamples: int\n",
    "        Number of samples to evaluate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float, float\n",
    "        Inception Score, FID\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load scorenet model\n",
    "    scorenet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "    #scorenet.load_state_dict(torch.load(ScoreNet_model_ckpt))\n",
    "\n",
    "\n",
    "    # Load DDPM model\n",
    "    model = DDPM_class(scorenet, T=T, x0_parameterization=False, use_low_discrepancy_sampler=False).to(device)\n",
    "\n",
    "    # Load model checkpoint\n",
    "    model.load_state_dict(torch.load(model_ckpt), strict=False)\n",
    "    \n",
    "    # Generate samples\n",
    "    samples = model.sample((nsamples,28*28)).cpu()\n",
    "    \n",
    "    # Map pixel values back from [-1,1] to [0,1]\n",
    "    samples = (samples+1)/2 \n",
    "    samples = samples.clamp(0.0, 1.0)\n",
    "\n",
    "    generated_samples = samples.reshape(-1, 1, 28, 28)\n",
    "\n",
    "    real_samples = []\n",
    "    for x, _ in dataloader:\n",
    "        real_samples.append(x.reshape(-1, 1, 28, 28))\n",
    "        if len(real_samples)*batch_size >= nsamples:\n",
    "            break\n",
    "    real_samples = torch.cat(real_samples, dim=0)[:nsamples].to(device)\n",
    "\n",
    "    #add 3 channels to generated and real samples\n",
    "    generated_samples = torch.cat([generated_samples, generated_samples, generated_samples], dim=1)\n",
    "    real_samples = torch.cat([real_samples, real_samples, real_samples], dim=1)\n",
    "\n",
    "    print(generated_samples.shape)\n",
    "    print(real_samples.shape)\n",
    "\n",
    "    #convert to float tensor\n",
    "    generated_samples = generated_samples.float()\n",
    "    real_samples = real_samples.float()\n",
    "\n",
    "    # Calculate Inception Score and FID\n",
    "    inception_mean, inception_std, FID =  calculate_inception_score_and_fid(\n",
    "    generated_samples,\n",
    "    real_samples,\n",
    "    batch_size=32,\n",
    "    device=device,\n",
    "    resize=True,\n",
    "    )\n",
    "\n",
    "    return inception_mean, inception_std, FID\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sk/6zwfz76x0z34qpjtwyy0r7y40000gn/T/ipykernel_48298/1864268220.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_ckpt), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 3, 28, 28])\n",
      "torch.Size([1000, 3, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcusdreisler/miniconda3/envs/PML/lib/python3.12/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: Metric `InceptionScore` will save all extracted features in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"/Users/marcusdreisler/Documents/phd/courses/PML-project/rfmt/model_discrete_ddpm_baseline_2_99.pt\"\n",
    "\n",
    "dataloader = dataloader_train\n",
    "\n",
    "inc_mean, inc_std, FID = evaluate(dataloader, DDPM, model_ckpt, device = \"cpu\", nsamples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.8744), tensor(0.0601), 115.08816528320312)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inc_mean, inc_std, FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms, utils\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from diffusion_utils import gaussian_nll, pred_xstart_from_eps\n",
    "from functions import normal_kl, discretized_gaussian_loglik, flat_mean\n",
    "\n",
    "class DDPM2(nn.Module):\n",
    "\n",
    "    def __init__(self, network, T=1000, beta_1=1e-4, beta_T=2e-2, x0_parameterization=False, use_low_discrepancy_sampler=False):\n",
    "        \"\"\"\n",
    "        Initialize Denoising Diffusion Probabilistic Model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        network: nn.Module\n",
    "            The inner neural network used by the diffusion process. Typically a Unet.\n",
    "        beta_1: float\n",
    "            beta_t value at t=1 \n",
    "        beta_T: [float]\n",
    "            beta_t value at t=T (last step)\n",
    "        T: int\n",
    "            The number of diffusion steps.\n",
    "        \"\"\"\n",
    "        \n",
    "        super(DDPM2, self).__init__()\n",
    "\n",
    "        self.x0_parameterization = x0_parameterization\n",
    "        self.use_low_discrepancy_sampler = use_low_discrepancy_sampler\n",
    "\n",
    "        # Normalize time input before evaluating neural network\n",
    "        # Reshape input into image format and normalize time value before sending it to network model\n",
    "        self._network = network\n",
    "        self.network = lambda x, t: (self._network(x.reshape(-1, 1, 28, 28), \n",
    "                                                   (t.float() / T))\n",
    "                                    ).reshape(-1, 28*28)\n",
    "\n",
    "        # Total number of time steps\n",
    "        self.T = T\n",
    "\n",
    "        # Registering as buffers to ensure they get transferred to the GPU automatically\n",
    "        self.register_buffer(\"beta\", torch.linspace(beta_1, beta_T, T+1))\n",
    "        self.register_buffer(\"alpha\", 1 - self.beta)\n",
    "        self.register_buffer(\"alpha_bar\", self.alpha.cumprod(dim=0))\n",
    "        self.register_buffer(\"beta_bar\", self.beta.cumprod(dim=0))\n",
    "        self.register_buffer(\"sqrt_alphas_bar\", torch.sqrt(self.alpha_bar))\n",
    "        self.register_buffer(\"sqrt_one_minus_alphas_bar\", torch.sqrt(1 - self.alpha_bar))\n",
    "\n",
    "        # q(x_{t-1} | x_t, x_0)\n",
    "        self.alphas_bar = self.alpha_bar\n",
    "        self.betas = self.beta\n",
    "        self.alphas = self.alpha\n",
    "        self.betas_bar = self.beta_bar\n",
    "        alphas_bar_prev = torch.cat([torch.tensor([1.], dtype=torch.float32, device=self.beta.device), self.alpha_bar[:-1]])\n",
    "        sqrt_alphas_bar_prev = torch.sqrt(alphas_bar_prev)\n",
    "        self.sqrt_recip_alphas_bar = torch.sqrt(1. / self.alphas_bar)\n",
    "        self.sqrt_recip_m1_alphas_bar = torch.sqrt(1. / self.alphas_bar - 1.)  # m1: minus 1\n",
    "        self.posterior_var = self.betas * (1. - alphas_bar_prev) / (1. - self.alphas_bar)\n",
    "        self.posterior_logvar_clipped = torch.log(torch.cat([self.posterior_var[1:2], self.posterior_var[1:]]))\n",
    "        self.posterior_mean_coef1 = self.betas * sqrt_alphas_bar_prev / (1. - self.alphas_bar)\n",
    "        self.posterior_mean_coef2 = torch.sqrt(self.alphas) * (1. - alphas_bar_prev) / (1. - self.alphas_bar)\n",
    "\n",
    "    def forward_diffusion(self, x0, t, epsilon):\n",
    "        '''\n",
    "        q(x_t | x_0)\n",
    "        Forward diffusion from an input datapoint x0 to an xt at timestep t, provided a N(0,1) noise sample epsilon. \n",
    "        Note that we can do this operation in a single step\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            x value at t=0 (an input image)\n",
    "        t: torch.Tensor\n",
    "            step index [batch_size]\n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t\n",
    "        ''' \n",
    "\n",
    "        mean = torch.sqrt(self.alpha_bar[t]) * x0\n",
    "        std = torch.sqrt(1 - self.alpha_bar[t])\n",
    "        \n",
    "        return mean + std * epsilon\n",
    "\n",
    "    def reverse_diffusion(self, xt, t, epsilon):\n",
    "        \"\"\"\n",
    "        p(x_{t-1} | x_t)\n",
    "        Single step in the reverse direction, from x_t (at timestep t) to x_{t-1}, provided a N(0,1) noise sample epsilon.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xt: torch.tensor\n",
    "            x value at step t\n",
    "        t: torch.Tensor\n",
    "            step index [batch_size]\n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t-1\n",
    "        \"\"\"\n",
    "        \n",
    "        # Equation 11 in Ho et al., 2020\n",
    "        model_mean, _, model_logvar = self.p_mean_var(\n",
    "            x_t=x_t, t=t, clip_denoised=False, return_pred=False)\n",
    "\n",
    "        # Compute standard deviation\n",
    "        std = torch.sqrt(((1 - self.alpha_bar[t-1]) / (1 - self.alpha_bar[t])) * self.beta[t])\n",
    "        \n",
    "        return model_mean + std * epsilon\n",
    "\n",
    "    def reverse_diffusion_x0_parameterization(self, xt, t, epsilon):\n",
    "        \"\"\"\n",
    "        p(x_{t-1} | x_t)\n",
    "        Single step in the reverse direction, from x_t (at timestep t) to x_{t-1}, provided a N(0,1) noise sample epsilon.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        xt: torch.tensor\n",
    "            x value at step t\n",
    "        t: torch.Tensor\n",
    "            step index [batch_size]\n",
    "        epsilon:\n",
    "            noise sample\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            image at timestep t-1\n",
    "        \"\"\"\n",
    "\n",
    "        # Network now predicts the initial image x0\n",
    "        estimated_x0 = self.network(xt, t)\n",
    "\n",
    "        alpha = self._extract(self.alpha, t, xt)          # Shape: [batch_size, 1]\n",
    "        alpha_bar = self._extract(self.alpha_bar, t, xt)  # Shape: [batch_size, 1]\n",
    "        alpha_bar_prev = self._extract(self.alpha_bar, t-1, xt)  # Shape: [batch_size, 1]\n",
    "        beta = self._extract(self.beta, t, xt)            # Shape: [batch_size, 1]\n",
    "\n",
    "        # Equation 6+7 in Ho et al., 2020\n",
    "\n",
    "        beta_tilde = ((1 - alpha_bar_prev) / (1 - alpha_bar)) * beta  # Shape: [batch_size, 1]\n",
    "        std = torch.sqrt(beta_tilde)  # Shape: [batch_size, 1]\n",
    "\n",
    "        coeff1 = torch.sqrt(alpha_bar_prev) / (1 - alpha_bar) * beta  # Shape: [batch_size, 1]\n",
    "        coeff2 = torch.sqrt(alpha) * (1 - alpha_bar_prev) / (1 - alpha_bar)  # Shape: [batch_size, 1]\n",
    "\n",
    "        mean = coeff1 * estimated_x0 + coeff2 * xt  # Shape: [batch_size, 784]\n",
    "\n",
    "        return mean + std * epsilon  # Shape: [batch_size, 784]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, shape):\n",
    "        \"\"\"\n",
    "        Sample from diffusion model (Algorithm 2 in Ho et al., 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        shape: tuple\n",
    "            Specify shape of sampled output. For MNIST: (nsamples, 28*28)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor\n",
    "            sampled image            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Sample xT: Gaussian noise\n",
    "        xT = torch.randn(shape).to(self.beta.device)\n",
    "\n",
    "        xt = xT\n",
    "        for t in range(self.T, 0, -1):\n",
    "            noise = torch.randn_like(xT) if t > 1 else torch.zeros_like(xT)\n",
    "            t_tensor = torch.full((xt.shape[0],), t, dtype=torch.long, device=self.beta.device)\n",
    "            if self.x0_parameterization:\n",
    "                xt = self.reverse_diffusion_x0_parameterization(xt, t_tensor, noise)\n",
    "            else:         \n",
    "                xt = self.reverse_diffusion(xt, t_tensor, noise)\n",
    "\n",
    "        return xt\n",
    "\n",
    "    def low_discrepancy_t_sampler(self, batch_size, device):\n",
    "        k = batch_size\n",
    "        u0 = np.random.uniform(0, 1)\n",
    "\n",
    "        # ti = mod(u0 + i/k, 1) for i=0,1,...,k-1\n",
    "        ti = torch.fmod(torch.arange(0, k, device=device, dtype=torch.float32) / k + u0, 1)\n",
    "        return (ti * self.T).long()\n",
    "\n",
    "    def elbo_simple(self, x0):\n",
    "        \"\"\"\n",
    "        ELBO training objective (Algorithm 1 in Ho et al., 2020)\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            Input image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            ELBO value            \n",
    "        \"\"\"\n",
    "\n",
    "        if self.use_low_discrepancy_sampler:\n",
    "            t = self.low_discrepancy_t_sampler(x0.shape[0], x0.device)\n",
    "        else:\n",
    "            # Sample time step t as a 1D tensor\n",
    "            t = torch.randint(1, self.T, (x0.shape[0],), device=x0.device)\n",
    "        \n",
    "        # Sample noise\n",
    "        epsilon = torch.randn_like(x0)\n",
    "\n",
    "        # Forward diffusion to produce image at step t\n",
    "        xt = self.forward_diffusion(x0, t, epsilon)\n",
    "        \n",
    "        return -nn.MSELoss(reduction='mean')(epsilon, self.network(xt, t))\n",
    "\n",
    "    def elbo_simple_x0_reparameterization(self, x0):\n",
    "        \"\"\"\n",
    "        ELBO training objective (Algorithm 1 in Ho et al., 2020), modified. Network tries to predict the noise, but the loss is taken in \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x0: torch.tensor\n",
    "            Input image\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            ELBO value            \n",
    "        \"\"\"\n",
    "\n",
    "        # Sample time step t as a 1D tensor\n",
    "        t = torch.randint(1, self.T, (x0.shape[0],), device=x0.device)\n",
    "        \n",
    "        # Sample noise\n",
    "        epsilon = torch.randn_like(x0)\n",
    "\n",
    "        # Forward diffusion to produce image at step t\n",
    "        xt = self.forward_diffusion(x0, t, epsilon)\n",
    "\n",
    "        estimated_x0 = self.network(xt, t)\n",
    "        \n",
    "        return -nn.MSELoss(reduction='mean')(x0, estimated_x0)\n",
    "\n",
    "    def loss(self, x0):\n",
    "        \"\"\"\n",
    "        Loss function. Just the negative of the ELBO.\n",
    "        \"\"\"\n",
    "        if self.x0_parameterization:\n",
    "            return -self.elbo_simple_x0_reparameterization(x0).mean()\n",
    "        else:\n",
    "            return -self.elbo_simple(x0).mean()\n",
    "\n",
    "    # === log likelihood ===\n",
    "    # bpd: bits per dimension\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract(arr, t, x, dtype=torch.float32, device=torch.device(\"cpu\"), ndim=2):\n",
    "        \"\"\"\n",
    "        Extracts the values from `arr` at indices `t` and reshapes them for broadcasting.\n",
    "\n",
    "        Parameters:\n",
    "            arr (torch.Tensor): Tensor from which to extract values. Shape: [T+1].\n",
    "            t (torch.Tensor): Time step indices. Shape: [batch_size].\n",
    "            x (torch.Tensor): Reference tensor for device and dtype.\n",
    "            dtype (torch.dtype): Desired data type of the output.\n",
    "            device (torch.device): Desired device of the output.\n",
    "            ndim (int): Number of dimensions for the output tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Extracted and reshaped tensor. Shape: [batch_size, 1].\n",
    "        \"\"\"\n",
    "        if x is not None:\n",
    "            dtype = x.dtype\n",
    "            device = x.device\n",
    "            ndim = x.ndim\n",
    "        out = torch.as_tensor(arr, dtype=dtype, device=device).gather(0, t)\n",
    "        return out.reshape((-1, ) + (1, ) * (ndim - 1))\n",
    "\n",
    "    def q_mean_var(self, x_0, t):\n",
    "        mean = self._extract(self.sqrt_alphas_bar, t, x_0) * x_0\n",
    "        var = self._extract(1. - self.alphas_bar, t, x_0)\n",
    "        logvar = self._extract(torch.log(1 - self.alphas_bar), t, x_0)\n",
    "        return mean, var, logvar\n",
    "\n",
    "    def q_sample(self, x_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        coef1 = self._extract(self.sqrt_alphas_bar, t, x_0)\n",
    "        coef2 = self._extract(self.sqrt_one_minus_alphas_bar, t, x_0)\n",
    "        return coef1 * x_0 + coef2 * noise\n",
    "\n",
    "    def q_posterior_mean_var(self, x_0, x_t, t):\n",
    "        posterior_mean_coef1 = self._extract(self.posterior_mean_coef1, t, x_0)\n",
    "        posterior_mean_coef2 = self._extract(self.posterior_mean_coef2, t, x_0)\n",
    "        posterior_mean = posterior_mean_coef1 * x_0 + posterior_mean_coef2 * x_t\n",
    "        posterior_var = self._extract(self.posterior_var, t, x_0)\n",
    "        posterior_logvar = self._extract(self.posterior_logvar_clipped, t, x_0)\n",
    "        return posterior_mean, posterior_var, posterior_logvar\n",
    "\n",
    "    def _loss_term_bpd(self, denoise_fn, x_0, x_t, t, clip_denoised, return_pred):\n",
    "        # Calculate L_t\n",
    "        # t = 0: negative log likelihood of decoder, -log p(x_0 | x_1)\n",
    "        # t > 0: variational lower bound loss term, KL term\n",
    "        true_mean, _, true_logvar = self.q_posterior_mean_var(x_0=x_0, x_t=x_t, t=t)\n",
    "        model_mean, model_var, model_logvar, pred_x0 = self.p_mean_var(\n",
    "            x_t=x_t, t=t, clip_denoised=clip_denoised, return_pred=True)\n",
    "        kl = normal_kl(true_mean, true_logvar, model_mean, model_logvar)\n",
    "        kl = flat_mean(kl) / math.log(2.)  # Convert from nats to bits\n",
    "        decoder_nll = discretized_gaussian_loglik(x_0, model_mean, log_scale=0.5 * model_logvar).neg()\n",
    "        decoder_nll = flat_mean(decoder_nll) / math.log(2.)\n",
    "        output = torch.where(t > 0, kl, decoder_nll)\n",
    "        return (output, pred_x0) if return_pred else output\n",
    "\n",
    "    def p_mean_var(self, x_t, t, clip_denoised=True, return_pred=False):\n",
    "        \"\"\"\n",
    "        Compute the mean and variance of p(x_{t-1} | x_t)\n",
    "\n",
    "        Parameters:\n",
    "            x_t (torch.Tensor): Current sample at time step t. Shape: [batch_size, 784].\n",
    "            t (torch.Tensor): Time step indices. Shape: [batch_size].\n",
    "            clip_denoised (bool): Whether to clamp the predicted x0 to [-1, 1].\n",
    "            return_pred (bool): Whether to return the predicted x0.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - mean (torch.Tensor): Mean of p(x_{t-1} | x_t). Shape: [batch_size, 784].\n",
    "                - var (torch.Tensor): Variance of p(x_{t-1} | x_t). Shape: [batch_size, 1].\n",
    "                - logvar (torch.Tensor): Log variance of p(x_{t-1} | x_t). Shape: [batch_size, 1].\n",
    "                - pred_x0 (torch.Tensor, optional): Predicted x0, if `return_pred` is True. Shape: [batch_size, 784].\n",
    "        \"\"\"\n",
    "        if self.x0_parameterization:\n",
    "            # Network predicts x0 directly\n",
    "            pred_x0 = self.network(x_t, t)  # Shape: [batch_size, 784]\n",
    "            if clip_denoised:\n",
    "                pred_x0 = pred_x0.clamp(-1., 1.)\n",
    "            \n",
    "            # Compute the posterior mean and variance using the predicted x0\n",
    "            mean, var, logvar = self.q_posterior_mean_var(x_0=pred_x0, x_t=x_t, t=t)  # Shapes: [batch_size, 784], [batch_size, 1], [batch_size, 1]\n",
    "        else:\n",
    "            # Network predicts the noise epsilon\n",
    "            epsilon_theta = self.network(x_t, t)  # Shape: [batch_size, 784]\n",
    "            \n",
    "            # Extract coefficients with shape [batch_size, 1] for broadcasting\n",
    "            alpha_t = self._extract(self.alpha, t, x_t)          # Shape: [batch_size, 1]\n",
    "            beta_t = self._extract(self.beta, t, x_t)            # Shape: [batch_size, 1]\n",
    "            alpha_bar_t = self._extract(self.alpha_bar, t, x_t)  # Shape: [batch_size, 1]\n",
    "            \n",
    "            # Compute the mean of p(x_{t-1} | x_t) using the predicted epsilon\n",
    "            mean = (1. / torch.sqrt(alpha_t)) * (\n",
    "                x_t - (beta_t / torch.sqrt(1 - alpha_bar_t)) * epsilon_theta\n",
    "            )  # Shape: [batch_size, 784]\n",
    "            \n",
    "            # Extract the variance and log variance with shape [batch_size, 1]\n",
    "            var = self._extract(self.posterior_var, t, x_t)           # Shape: [batch_size, 1]\n",
    "            logvar = self._extract(self.posterior_logvar_clipped, t, x_t)  # Shape: [batch_size, 1]\n",
    "            \n",
    "            if clip_denoised:\n",
    "                # Estimate x0 from the predicted epsilon\n",
    "                pred_x0 = (x_t - torch.sqrt(1 - alpha_bar_t) * epsilon_theta) / torch.sqrt(alpha_bar_t)  # Shape: [batch_size, 784]\n",
    "                pred_x0 = pred_x0.clamp(-1., 1.)\n",
    "            else:\n",
    "                pred_x0 = None\n",
    "\n",
    "        if return_pred:\n",
    "            return mean, var, logvar, pred_x0\n",
    "        else:\n",
    "            return mean, var, logvar\n",
    "\n",
    "    def train_losses(self, denoise_fn, x_0, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_0)\n",
    "        x_t = self.q_sample(x_0, t=t)  # Shape: [batch_size, 784]\n",
    "\n",
    "        # Calculate the loss\n",
    "        # kl: weighted\n",
    "        # mse: unweighted\n",
    "        if self.loss_type == \"kl\":\n",
    "            losses = self._loss_term_bpd(\n",
    "                denoise_fn, x_0=x_0, x_t=x_t, t=t, clip_denoised=False, return_pred=False)\n",
    "        elif self.loss_type == \"mse\":\n",
    "            assert self.model_var_type != \"learned\"\n",
    "            if self.model_mean_type == \"mean\":\n",
    "                target = self.q_posterior_mean_var(x_0=x0, x_t=x_t, t=t)[0]\n",
    "            elif self.model_mean_type == \"x_0\":\n",
    "                target = x0\n",
    "            elif self.model_mean_type == \"eps\":\n",
    "                target = noise\n",
    "            else:\n",
    "                raise NotImplementedError(self.model_mean_type)\n",
    "            model_out = denoise_fn(x_t, t)\n",
    "            losses = flat_mean((target - model_out).pow(2))\n",
    "        else:\n",
    "            raise NotImplementedError(self.loss_type)\n",
    "\n",
    "        return losses\n",
    "\n",
    "    # def _prior_bpd(self, x_0):\n",
    "    #     B, T = len(x_0), self.T\n",
    "    #     t = (T - 1) * torch.ones((B,), dtype=torch.long, device=x_0.device)\n",
    "    #     T_mean, _, T_logvar = self.q_mean_var(\n",
    "    #         x_0=x_0, t=t)\n",
    "    #     kl_prior = normal_kl(T_mean, T_logvar, mean2=0., logvar2=0.)\n",
    "    #     return flat_mean(kl_prior) / math.log(2.)\n",
    "\n",
    "    def _prior_bpd(self, x_0):\n",
    "        B, T = len(x_0), self.T\n",
    "        t = (T - 1) * torch.ones((B,), dtype=torch.long, device=x_0.device)\n",
    "        T_mean, _, T_logvar = self.q_mean_var(x_0=x_0, t=t)\n",
    "        \n",
    "        # Create tensors for mean2 and logvar2\n",
    "        mean2 = torch.zeros_like(T_mean)\n",
    "        logvar2 = torch.zeros_like(T_logvar)\n",
    "        \n",
    "        # Calculate KL divergence\n",
    "        kl_prior = normal_kl(T_mean, T_logvar, mean2=mean2, logvar2=logvar2)\n",
    "        \n",
    "        return flat_mean(kl_prior) / math.log(2.)\n",
    "\n",
    "\n",
    "    def calc_all_bpd(self, denoise_fn, x_0, clip_denoised=True):\n",
    "        B = x_0.shape[0]  # Ensure B is the batch size\n",
    "        T = self.T\n",
    "        t = torch.empty((B,), dtype=torch.long, device=x_0.device)  # Use long for indexing\n",
    "        losses = torch.zeros((B, T), dtype=torch.float32, device=x_0.device)  # Ensure device consistency\n",
    "        mses = torch.zeros((B, T), dtype=torch.float32, device=x_0.device)  \n",
    "\n",
    "        for ti in range(T - 1, -1, -1):\n",
    "            t.fill_(ti)\n",
    "            x_t = self.q_sample(x_0, t=t)\n",
    "            loss, pred_x0 = self._loss_term_bpd(\n",
    "                denoise_fn, x_0, x_t=x_t, t=t, clip_denoised=clip_denoised, return_pred=True)\n",
    "            losses[:, ti] = loss\n",
    "            mses[:, ti] = flat_mean((pred_x0 - x_0).pow(2))\n",
    "\n",
    "        prior_bpd = self._prior_bpd(x_0)\n",
    "        total_bpd = torch.sum(losses, dim=1) + prior_bpd\n",
    "        return total_bpd, losses, prior_bpd, mses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sk/6zwfz76x0z34qpjtwyy0r7y40000gn/T/ipykernel_72341/1065237279.py:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  DDPM_model.load_state_dict(torch.load(model_ckpt), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 784])\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "mnist_unet = ScoreNet((lambda t: torch.ones(1).to(device)))\n",
    "\n",
    "T = 1000\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x + torch.rand(x.shape)/255),\n",
    "    transforms.Lambda(lambda x: (x-0.5)*2.0),\n",
    "    transforms.Lambda(lambda x: x.flatten())\n",
    "])\n",
    "\n",
    "dataloader_ = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data', download=True, train=True, transform=transform),\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "\n",
    "\n",
    "DDPM_model = DDPM2(mnist_unet, T=T, x0_parameterization=False, use_low_discrepancy_sampler=True).to(device)\n",
    "model_ckpt = \"/Users/marcusdreisler/Documents/phd/courses/PML-project/rfmt/model_discrete_ddpm_low_dis.pt\"\n",
    "nsamples = 1000\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "DDPM_model.load_state_dict(torch.load(model_ckpt), strict=False)\n",
    "\n",
    "real_samples = []\n",
    "for x, _ in dataloader_:\n",
    "    real_samples.append(x)\n",
    "    if len(real_samples)*batch_size >= nsamples:\n",
    "        break\n",
    "\n",
    "real_samples = torch.cat(real_samples, dim=0)[:batch_size].to(device)\n",
    "print(real_samples.shape)\n",
    "#calc bpd\n",
    "\n",
    "DDPM_model.eval()\n",
    "# Calculate BPD\n",
    "total_bpd, losses, prior_bpd, mses = DDPM_model.calc_all_bpd(DDPM_model.network, real_samples, clip_denoised=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.9898, 3.6926, 3.0691, 3.6941, 2.5601, 3.0409, 2.9990, 2.3733, 3.0140,\n",
       "        3.2997], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_bpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0732598"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(total_bpd.detach().numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2bffd3855f5744f588d5be1e5c4aed3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b61ee9c62994863b718c086d4182f44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "645d91e4bb974b1196be61b5077c9dc5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_78dc714c7aa347fb9fc41abf420222d9",
       "IPY_MODEL_c1260f271df547fbb2a158ff6b3a3ff4",
       "IPY_MODEL_e7313fdbb70442f4867644dfc85c3bcc"
      ],
      "layout": "IPY_MODEL_a501588b5eb0494996dfb136565365ca"
     }
    },
    "78dc714c7aa347fb9fc41abf420222d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89c68eded05d441daf94d145addb5ece",
      "placeholder": "​",
      "style": "IPY_MODEL_2bffd3855f5744f588d5be1e5c4aed3e",
      "value": "Training:  24%"
     }
    },
    "89c68eded05d441daf94d145addb5ece": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b905c5b2ad846ca837bd20cce2bf094": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a501588b5eb0494996dfb136565365ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aca161ff9f4b4a20b1457a8ee864f150": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b1416c32c4af4fe9a3c3fdcc5f33aca0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1260f271df547fbb2a158ff6b3a3ff4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b61ee9c62994863b718c086d4182f44",
      "max": 5900,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8b905c5b2ad846ca837bd20cce2bf094",
      "value": 1394
     }
    },
    "e7313fdbb70442f4867644dfc85c3bcc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1416c32c4af4fe9a3c3fdcc5f33aca0",
      "placeholder": "​",
      "style": "IPY_MODEL_aca161ff9f4b4a20b1457a8ee864f150",
      "value": " 1393/5900 [05:15&lt;16:04,  4.67it/s, epoch=12/50, loss=⠀   2400.1270]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
